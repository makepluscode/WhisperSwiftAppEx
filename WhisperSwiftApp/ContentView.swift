//
//  ContentView.swift
//  WhisperSwiftApp
//
//  Created by makepluscode on 7/19/25.
//

import SwiftUI

struct ContentView: View {
    @State private var transcribedText: String = ""
    @State private var isTranscribing: Bool = false
    
    var body: some View {
        VStack(spacing: 30) {
            // ì•„ì´ì½˜ê³¼ íƒ€ì´í‹€
            VStack(spacing: 20) {
                Image(systemName: "waveform")
                    .font(.system(size: 60))
                    .foregroundColor(.blue)
                
                Text("Whisper Speech-to-Text")
                    .font(.title)
                    .fontWeight(.bold)
            }
            
            // ê²°ê³¼ í…ìŠ¤íŠ¸ ì˜ì—­
            ScrollView {
                VStack(alignment: .leading, spacing: 10) {
                    if isTranscribing {
                        HStack {
                            ProgressView()
                                .scaleEffect(0.8)
                            Text("Transcribing...")
                                .foregroundColor(.gray)
                        }
                    } else if !transcribedText.isEmpty {
                        Text("Transcription Result:")
                            .font(.headline)
                            .foregroundColor(.blue)
                        
                        Text(transcribedText)
                            .font(.body)
                            .padding()
                            .background(Color.gray.opacity(0.1))
                            .cornerRadius(8)
                    } else {
                        Text("Press the button below to transcribe test.wav")
                            .font(.body)
                            .foregroundColor(.gray)
                            .multilineTextAlignment(.center)
                    }
                }
                .padding()
            }
            .frame(maxWidth: .infinity, maxHeight: .infinity)
            
            // ë²„íŠ¼
            Button(action: {
                transcribeAudio()
            }) {
                HStack {
                    Image(systemName: "play.fill")
                        .foregroundColor(.white)
                    Text("Transcribe test.wav")
                        .foregroundColor(.white)
                        .fontWeight(.semibold)
                }
                .padding()
                .frame(maxWidth: .infinity)
                .background(Color.blue)
                .cornerRadius(10)
            }
            .disabled(isTranscribing)
            .padding(.horizontal)
        }
        .padding()
    }
    
    private func transcribeAudio() {
        print("ğŸ¤ Transcribe button pressed!")
        isTranscribing = true
        transcribedText = ""
        
        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ transcribe ì‹¤í–‰
        DispatchQueue.global(qos: .userInitiated).async {
            print("ğŸ”„ Starting transcription in background...")
            
            // ì„ì‹œë¡œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì‚¬ìš© (C í•¨ìˆ˜ ì—°ê²° ì „ê¹Œì§€)
            let result = "Hello, this is a test transcription result from the audio file."
            print("âœ… Transcription completed: \(result)")
            
            // UI ì—…ë°ì´íŠ¸ëŠ” ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ
            DispatchQueue.main.async {
                print("ğŸ“± Updating UI with result...")
                isTranscribing = false
                transcribedText = result
                print("ğŸ‰ UI updated successfully!")
            }
        }
    }
}

#Preview {
    ContentView()
}
